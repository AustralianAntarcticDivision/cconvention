---
title: "Parsing the CCAMLR Convention text."
author: "S. Wotherspoon"
date: "18 September 2016"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Parsing convention text}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## CCAMLR Convention Areas

We are interested in parsing the [technical description](https://www.ccamlr.org/en/organisation/convention-area-%E2%80%93-technical-description#48) of the CCAMLR convention areas.

## Parsing

Parsing is typically considered in two phases.  The "lexer" translates the input text into "tokens" - the basic particles of the language, and then in the second phase the "parser" assembles the sequence of tokens into higher order structure according to a grammar.  This is very efficient because the basic translation of input into tokens happens only once. Less sophisticated parsers will not have an explicit lexer, the tokenization is done as part of the grammar. 

Tools like Lex and Yacc construct complex state machines for parsing - Lex constructs the lexer, and Yacc the parser. The state machines generated by tools like these are extremely efficient, but the generated code is almost impossible to understand, and the parsers they generate can be difficult to debug.  

Parser combinators provide a modern, flexible alternative, but typically generate less efficient parsers. A parser combinator is higher order function that combines primitive parsers to form complex recursive descent parsers.

## Parser Combinators

There are numerous combinator libraries available, and for sophisticated parsers it would be best to use one of these.  But to illustrate the basics it is simple enough to write our own combinators.


### No Explicit Lexer

Consider first the case where there is no explicit lexer.

#### Primitive Parsers

We define a `parser` for a grammar to be a function accepts an input and if

* the input does not satisfies the grammar it returns `failure` (in our case `NULL`),

* otherwise it returns a list with elements 
    * `input` any unconsumed input, and
    * `result` the object constructed by the parse.
    
This function generates parsers that satisfy grammars defined by a regular expression.  All the hard work is done by the regular expression matching.  The value is returned as an S3 class to make it easier to write functions to format and evaluate the result.


```{r}
library(stringi)
library(Combin8R)

```


We can use this to define a parser for an degree, minute, second specification of a coordinate 
```{r}
## using this strange name for now because sp::as.character.DMS clashes otherwise
parserDMS1 <-
  Combin8R:::pRegex("DMS1",
              "^\\s*(\\d\\d?\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(E|W|N|S)")
```

These inputs parse
```{r}
parserDMS1("57Â°S")
parserDMS1("157Â°20'E")
parserDMS1("57Â°2'14\"N")
```
These fail
```{r}
parserDMS1("some text")
parserDMS1("57.33S")
```

Because the result is an S3 class, we can write a function to represent the parse results as a string
```{r}
## all in Combin8R
# formatAST <- function(x,indent=0) {
#   UseMethod("formatAST")
# }
# formatAST.default <- function(x,indent=0) {
#   pad <- paste(rep(" ",indent),collapse="")
#   paste(pad,deparse(x),sep="")
# }
# formatAST.pRegex <- function(x,indent=0) {
#   pad <- paste(rep(" ",indent),collapse="")
#   paste(pad,"[",class(x)[1],"; ",paste(x$value,collapse=" "),"]\n",sep="")
# }
```

So that
```{r}
p <- parserDMS1("57Â°20'14\"N")
cat(formatAST(p$result))
```

Or we can write translators specific to the class
```{r}
formatAST.DMS1 <- function(x,indent=0) {
  pad <- paste(rep(" ",indent),collapse="")
  paste0(pad,
         x$value[1],"Â°",
         if(!is.na(x$value[2])) 
           paste0(x$value[2],"'",
                  if(!is.na(x$value[3])) 
                    paste0(x$value[3],"\"")),
         x$value[4])
}
``` 
So now
```{r}
cat(formatAST(p$result))
```


We would also wish to write functions

* `checkAST` - a function for sanity checking the parse.  For example our DMS1 parser accepts a value of 80 for the minute field.  We could refine our regular expressions to detect this an an error, but it is often easier to let the parser to focus on syntax and perform more semantic checks in a second pass.

* `evalAST` - the parser will represent the parse as a tree, this function would translate the tree to a more useful representation in terms of R objects. The alternative is to have each parser generator function accept an additional argument that is a function that is used to transform the parsed values into a more useful R representation. 

#### Combinators

The parser `combinators` combine simple parsers to form more complex parsers.  The combinators produce recursive descent parsers - the parser recognizes language constructs by repeated trying to find specific sequences of simpler constructs.  A parser may be defined in terms of itself, provided we avoid "left recursion" where a construct is recognized by first recognizing a simper form of itself.

The alternation combinator creates a parser that succeeds on input that is accepted by any one of a number of simpler parsers.  Order is important here, as this version always returns the first match.  This means the grammar cannot be ambiguous in such a way that accepting the first matching pass at this stage might cause the parse to subsequently fail at a higher level, bu accepting a later match might succeed.  

```{r}
## Combin8R:::pAlt
# parserAlt <- function(tag,...) {
#   ps <- list(...)
#   function(input) {
#     ## Try each parser in turn
#     for(p in ps) {
#       ## Return first successful parse
#       if(!is.null(parse <- p(input)))
#         return(list(input=parse$input,
#                     result=structure(list(value=parse$result),class=c(tag,"pAlt"))))
#     }
#     ## All parses failed
#     NULL
#   }
# }
formatAST.pAlt <- function(x,indent=0) {
  pad <- paste(rep(" ",indent),collapse="")
  paste(pad,"||",class(x)[1],"; ",formatAST(x$value),sep="")
}
```

The sequence combinator creates a parser that succeeds on input accepted by a sequence of simpler parsers applied in succession

```{r}
## Combin8R:::pSeq
# parserSeq <- function(tag,...) {
#   ps <- list(...)
#   function(input) {
#     ## Try each parser in sequence, accumulating parse results
#     values <- vector(mode="list",length(ps))
#     for(k in seq_along(ps)) {
#       parse <- ps[[k]](input)
#       ## If any match fails, the sequence fails
#       if(is.null(parse)) return(NULL)
#       values[[k]] <- parse$result
#       input <- parse$input
#     }
#     list(input=input,
#          result=structure(list(value=values),class=c(tag,"pSeq")))
#   }
# }
formatAST.pSeq <- function(x,indent=0) {
  pad <- paste(rep(" ",indent),collapse="")
  paste(pad,"(",class(x)[1],";\n",
        paste(sapply(x$value,formatAST,indent=indent+2),collapse=""),
        pad,")",sep="",collapse="")
}
```

The many combinator creates a parser that succeeds on input for which a simpler parser succeeds zero or more times when applied in succession.  It cannot fail, and it always accepts the longest match.
```{r}
## Combin8R:::pMany
# parserMany <- function(tag,p) {
#   function(input) {
#     k <- 0
#     values <- list()
#     ## Repeatedly try parser, accumulating parse results
#     repeat {
#       parse <- p(input)
#       ## Finish when a parse fails
#       if(is.null(parse)) break
#       values[[k <- k+1]] <- parse$result
#       input <- parse$input
#     }
#     list(input=input,
#          result=structure(list(value=values),class=c(tag,"pMany")))
#   }
# }
formatAST.pMany <- function(x,indent=0) {
  pad <- paste(rep(" ",indent),collapse="")
  paste(pad,"*{",class(x)[1],";\n",
        paste(sapply(x$value,formatAST,indent=indent+2),collapse=""),
        pad,"}",sep="")
}
```

A minor variant is the some parser that succeeds on input for which a simpler parser succeeds zero or more times when applied in succession.  It can be constructed from `parserSeq` and `parserMany` but is used often enough to get its own (more efficient implementation). 

```{r}
##Combin8R:::pSome
# parserSome <- function(tag,p) {
#   function(input) {
#     parse <- p(input)
#     if(is.null(parse)) return(NULL)
#     k <- 1
#     values <- list(parse$result)
#     ## Continue trying parser, accumulating parse results
#     repeat {
#       parse <- p(input)
#       ## Finish when a parse fails
#       if(is.null(parse)) break
#       values[[k <- k+1]] <- parse$result
#       input <- parse$input
#     }
#     list(input=input,
#          result=structure(list(value=values),class=c(tag,"pSome")))
#   }
# }
formatAST.pSome <- function(x,indent=0) {
  pad <- paste(rep(" ",indent),collapse="")
  paste(pad,"+{",class(x)[1],";\n",
      paste(sapply(x$value,formatAST,indent=indent+2),collapse=""),
      pad,"}",sep="")
}
```


So with this framework we can write
```{r}
parserStartPath0 <-
  Combin8R::pSeq("StartPath",
           Combin8R:::pRegex("StartText","^The waters bounded by a line starting at"),
           parserDMS1,
           parserDMS1,
           Combin8R:::pRegex("Semi","^;\\s*"))
```

```{r}
txt <- "The waters bounded by a line starting at 57Â°S 50Â°W; thence due east to 30Â°W longitude; thence due south to 64Â°S latitude; thence due west to 50Â°W longitude; thence due north to the starting point."
r <- parserStartPath0(txt)
r$input
cat(formatAST(r$result))
```


This would be fine for a very general parser where there is some expectation that the input text will contain errors, and we have some interest is locating the point at which the parse fails.  

But for your case it is probably simpler to treat whole clauses as single tokens.  If we create simple parsers for some common clauses
```{r}
## The waters bounded by a line starting at POINT
parserPathStart <-
  Combin8R:::pRegex("PathStart",
               "^\\s*The waters bounded by a line starting at (\\d\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(N|S) (\\d\\d?\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(E|W);\\s*")
## thence due DIRECTION to (PARALLEL|MERIDIAN)
parserPathLineLL <-
  Combin8R:::pRegex("PathLineLL",
              "^\\s*thence due (\\w+) to (\\d\\d?\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(N|S|E|W) (latitude|longitude);\\s*")
# thence DIRECTION to the point POINT
# thence to POINT 
parserPathLinePt <-
  Combin8R:::pRegex("PathLinePt",
              "^\\s*thence (?:(\\w+) )?to (?:the point )?(\\d\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(N|S), (\\d\\d?\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(E|W);\\s*")
## thence due DIRECTION to the starting point.
parserPathEnd <-
  Combin8R:::pRegex("PathEnd",
              "^\\s*thence due (\\w+) to the starting point.\\s*")
```

These can be combined to form a parser for the simpler description
```{r}
parserPath <- 
  pSeq("Path",
            parserPathStart,
            pSome("PathLineList",
                       pAlt("parserPathLine",
                                 parserPathLineLL,
                                 parserPathLinePt)),
            parserPathEnd)
```

```{r}
r <- parserPath(txt)
cat(formatAST(r$result))
```

```{r}
txt <- "The waters bounded by a line starting at 45Â°S 60Â°E; thence due south to 53Â°14'S latitude; thence east to the point 53Â°14'07\"S, 67Â°03'20\"E; thence to 52Â°42'28\"S, 68Â°05'31\"E; thence to 51Â°58'18\"S, 69Â°44'02\"E; thence to 51Â°24'32\"S, 71Â°12'29\"E; thence to 51Â°03'09\"S, 72Â°28'28\"E; thence to 50Â°54'23\"S, 72Â°49'21\"E; thence to 49Â°49'34\"S, 75Â°36'08\"E; thence to 49Â°24'07\"S, 76Â°42'17\"E; thence due east to 80Â°E longitude; thence due north to 45Â°S latitude; thence due west to the starting point."
r1 <- parserPath(txt)
cat(formatAST(r1$result))
```

This still needs more work to cater for clauses such as

* thence westward along the coast of the Antarctic Continent to 20Â°W longitude

* thence westward along and including the coast of the Antarctic Peninsula to 75Â°W longitude

* thence westward along the coast of the Antarctic Continent and northwards along the coast of the Antarctic Peninsula to 65Â°S latitude

* thence westward along the coast of the Antarctic Continent, the Weddell Sea and the Antarctic Peninsula to 70Â°W longitude, where the meridian meets the coast of Ellsworth Land

* thence north across George VI Sound and due north to 60Â°S latitude

* thence around the peninsula to 70Â°W longitude, where the meridian meets the coast of Ellsworth Land

* thence due west to the eastern coast of the Antarctic Peninsula

We could either put something together with parserRegex, or we could write a handcrafted parser for these more compex clauses.

This then has to be embedded into a larger parser that identifies

* Areas
* SubAreas
* Divisions
* Subdivisions
* Integrated Study Regions

## Explicit Lexer

If we were to make the lexing phase explicit, we would write a lexer that converts the input into a stream of tokens (or lexemes).  Part of the trick is that the tokens must be recognisable independent of context in which they are used.

The individual lexers are defined in terms of regular expression  
```{r}
library(stringi)
lexerRegexp <- function(tag,regex)
  function(input) {
    match <- stri_match_first_regex(input,regex)
    ## If the regex matched, consume input and return any captured
    ## groups as values
    if(!is.na(match[1,1]))
      list(input=substr(input,nchar(match[1,1])+1,nchar(input)),
           token=structure(list(values=match[1,-1]),class=c(tag,"lRegex")))
  }
print.lRegex <- function(x,...) {
  cat(class(x)[1],":",x$values)
}
formatAST.lRegex <- function(x,indent=0) {
  pad <- paste(rep(" ",indent),collapse="")
  paste(pad,"[",class(x)[1],"; ",paste(x$value,collapse=" "),"]\n",sep="")
}
```

The lexer generator is like a combination of `parseMany` and `parseSeq`
```{r}
makeLexer <- function(...) {
  ls <- list(...)
  tokens <- list()
  
  function(input) {
    k <- 0
    repeat {
      ## Return if input consumed
      if(nchar(input)==0) return(list(input=input,tokens=tokens))
      ## Call lexers
      for(l in ls)
        if(!is.null(lex <- l(input))) break
      ## If all failed return partial result
      if(is.null(lex)) return(list(input=input,tokens=tokens))
      ## Else store token and consume input
      tokens[[k <- k+1]] <- lex$token
      input <- lex$input
    }
  }
}
```


```{r}
lexer <- makeLexer(
  ## The waters bounded by a line starting at POINT
  lexerRegexp("PathStart",
              "^\\s*The waters bounded by a line starting at (\\d\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(N|S) (\\d\\d?\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(E|W);\\s*"),
  ## thence due DIRECTION to (PARALLEL|MERIDIAN)
  lexerRegexp("PathLineLL",
              "^\\s*thence due (\\w+) to (\\d\\d?\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(N|S|E|W) (latitude|longitude);\\s*"),
  # thence DIRECTION to the point POINT
  # thence to POINT 
  lexerRegexp("PathLinePt",
              "^\\s*thence (?:(\\w+) )?to (?:the point )?(\\d\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(N|S), (\\d\\d?\\d?)Â°(?:(\\d\\d?)'(?:(\\d\\d?)\")?)?(E|W);\\s*"),
  ## thence due DIRECTION to the starting point.
  lexerRegexp("PathEnd",
              "^\\s*thence due (\\w+) to the starting point.\\s*"))
```

```{r}
txt <- "The waters bounded by a line starting at 45Â°S 60Â°E; thence due south to 53Â°14'S latitude; thence east to the point 53Â°14'07\"S, 67Â°03'20\"E; thence to 52Â°42'28\"S, 68Â°05'31\"E; thence to 51Â°58'18\"S, 69Â°44'02\"E; thence to 51Â°24'32\"S, 71Â°12'29\"E; thence to 51Â°03'09\"S, 72Â°28'28\"E; thence to 50Â°54'23\"S, 72Â°49'21\"E; thence to 49Â°49'34\"S, 75Â°36'08\"E; thence to 49Â°24'07\"S, 76Â°42'17\"E; thence due east to 80Â°E longitude; thence due north to 45Â°S latitude; thence due west to the starting point."
lex <- lexer(txt)
lex$tokens
```

So the input text is converted to a list of tokens.  The `parserRegex` function becomes replaced with a function `parserToken` that checks the class of the token and consumes the matching token off the input stream
```{r}
parserToken <- function(token)
  function(input) {
    if(length(input) > 0 && class(input[[1]])[1] == token)
      list(input=input[-1],result=input[[1]])
  }
```
and the combinators stay more or less the same.
These can be combined to form a parser for the simpler description
```{r}
parserPath <- 
  pSeq("Path",
            parserToken("PathStart"),
            pSome("PathLineList",
                       pAlt("parserPathLine",
                                 parserToken("PathLineLL"),
                                 parserToken("PathLinePt"))),
            parserToken("PathEnd"))
```

```{r}
r2 <- parserPath(lex$tokens)
 cat(formatAST(r2$result))
```

Oddly, efficiency here comes down to the efficiency with which we can subset R's lists versus R's strings.  It is not clear that an explicit lexer will make things faster in R.
